# FizTorch: A Toy Tensor Library for Machine Learning

![Logo](assets/fiztorch.png)

## Introduction âœ¨
FizTorch is a toy implementation of a tensor library inspired by popular machine learning frameworks like PyTorch and TensorFlow. This project aims to create a basic tensor class with essential operations and functionality, helping you understand the core concepts behind tensor-based machine learning.

## Key Features ğŸ”‘
- â• Basic tensor operations: addition, subtraction, multiplication, division
- ğŸ§® Matrix multiplication
- ğŸ“ Handling of tensor shapes and sizes
- ğŸ”„ Gradient tracking and backpropagation
- ğŸ’¾ Serialization and persistence
- ğŸ§ª Unit testing and comprehensive documentation
- ğŸ§  Neural network modules and layers
- ğŸ“Š Data loading and batching
- ğŸ”§ Optimization algorithms

## Examples ğŸš€

### Basic Tensor Operations

```python
from ftensor import FTensor as ft

# Creating higher-dimensional tensors
a = ft([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])  # Shape: (2, 2, 2)
b = ft([[[1, 0], [0, 1]], [[1, 1], [1, 1]]])  # Shape: (2, 2, 2)

# Basic operations
print("a + b:\n", a + b)                          # Element-wise addition
print("a - b:\n", a - b)                          # Element-wise subtraction
print("a * b:\n", a * b)                          # Element-wise multiplication

# Dot product
result = a.dot(b)
print("a.dot(b):\n", result)                      # Dot product

# More operations...
```

### Neural Network Training

```python
import numpy as np
from ftensor import nn, optim, data, utils

# Create a simple dataset
X = np.random.randn(1000, 10)
y = np.random.randint(0, 2, (1000, 1))
dataset = data.Dataset(list(zip(X, y)))
dataloader = data.DataLoader(dataset, batch_size=32)

# Create a model
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 1),
    nn.Sigmoid()
)

# Define loss function and optimizer
loss_fn = lambda pred, target: ((pred - target) ** 2).mean()
optimizer = optim.Adam(model.parameters(), learning_rate=0.001)

# Train the model
utils.train(model, dataloader, loss_fn, optimizer, epochs=10)
print("Training completed!")
```

## Implementation Goals ğŸ¯

### Completed Functionalities âœ…

- âœ… **Higher-Dimensional Tensors**: Support for tensors of arbitrary dimensions.
- âœ… **Basic Operations**: Element-wise addition, subtraction, and multiplication.
- âœ… **Dot Product**: Implemented dot product functionality for tensor operations.
- âœ… **Flattening and Transposing**: Methods to flatten and transpose tensors.
- âœ… **Element-wise Operations**: Logarithm, exponential, softmax, and ReLU derivative.
- âœ… **Reshaping Tensors**: Reshape tensors to desired dimensions.
- âœ… **Advanced Tensor Manipulations**: Tensor summation over specified axes.
- âœ… **Neural Network Modules**: Implemented Linear, ReLU, and Sigmoid layers.
- âœ… **Optimizers**: Adam optimizer for parameter updates.
- âœ… **Data Handling**: Dataset and DataLoader classes for batch processing.
- âœ… **Training Utility**: Helper function for model training.

### Known Issues âš ï¸

- âš ï¸ **Broadcasting Support**: Some broadcasting operations require further debugging.

### Future Additions and Features ğŸš€

- ğŸ”² **Additional Neural Network Layers**: Convolutional, pooling, and recurrent layers.
- ğŸ”² **More Optimizers**: Implement SGD, RMSprop, and other optimization algorithms.
- ğŸ”² **Support for Sparse Tensors**: Enhance functionality to handle sparse tensor representations.
- ğŸ”² **GPU Acceleration**: Integrate support for GPU computations for performance improvement.
- ğŸ”² **Comprehensive Documentation**: Provide detailed usage examples and API documentation.
- ğŸ”² **Expanded Unit Testing**: Cover more edge cases and functionality.
- ğŸ”² **Performance Benchmarks**: Create benchmarks to evaluate performance against other frameworks.

## Getting Started ğŸš€
To get started, clone the repository and set up your development environment. You'll need Python 3.x installed on your system.

```bash
git clone https://github.com/ahammadnafiz/FizTorch.git
cd FizTorch
```

## Contributing ğŸ¤
Contributions are welcome! If you'd like to help, please follow these guidelines:

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a new branch for your feature or bug fix
3. âœï¸ Write your code and add tests
4. ğŸ“¬ Submit a pull request

## License
[MIT License](LICENSE)